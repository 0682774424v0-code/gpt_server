{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d68e213",
   "metadata": {},
   "source": [
    "# üé® Stable Diffusion WebUI for Google Colab\n",
    "\n",
    "**Complete image generation platform with API keys management, model downloads, and advanced features**\n",
    "\n",
    "This notebook provides:\n",
    "- ‚úÖ HuggingFace & Civitai API key configuration\n",
    "- ‚úÖ Checkpoint and LoRA model downloads\n",
    "- ‚úÖ Full image generation with advanced parameters\n",
    "- ‚úÖ Google Drive integration for backup\n",
    "- ‚úÖ GPU acceleration and memory optimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67eed76",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Initialize Dependencies and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ae2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q diffusers transformers accelerate safetensors\n",
    "!pip install -q huggingface-hub omegaconf einops requests\n",
    "!pip install -q opencv-python-headless pillow numpy scipy\n",
    "!pip install -q ipywidgets google-auth-oauthlib google-auth-httplib2 google-api-python-client\n",
    "!pip install -q flask flask-socketio python-socketio\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0e000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import PIL for image handling\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "\n",
    "# Import diffusers\n",
    "from diffusers import (\n",
    "    StableDiffusionPipeline,\n",
    "    StableDiffusionXLPipeline,\n",
    "    EulerDiscreteScheduler,\n",
    "    PNDMScheduler,\n",
    "    DDIMScheduler,\n",
    "    AutoencoderKL\n",
    ")\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "# Import huggingface hub\n",
    "from huggingface_hub import hf_hub_download, list_repo_files\n",
    "\n",
    "# Import ipywidgets for GUI\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "# Setup\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Create directories\n",
    "Path('./models/checkpoints').mkdir(parents=True, exist_ok=True)\n",
    "Path('./models/loras').mkdir(parents=True, exist_ok=True)\n",
    "Path('./models/vaes').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\n‚úÖ Directories created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe4c18",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ API Keys Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c0fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIKeysManager:\n",
    "    \"\"\"Manage API keys for HuggingFace and Civitai\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.hf_token = os.getenv('HF_TOKEN', '')\n",
    "        self.civitai_key = os.getenv('CIVITAI_API_KEY', '')\n",
    "        self.status = {'hf': False, 'civitai': False}\n",
    "    \n",
    "    def set_hf_token(self, token: str) -> bool:\n",
    "        \"\"\"Set HuggingFace token\"\"\"\n",
    "        if token.strip():\n",
    "            self.hf_token = token.strip()\n",
    "            os.environ['HF_TOKEN'] = self.hf_token\n",
    "            self.status['hf'] = True\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def set_civitai_key(self, key: str) -> bool:\n",
    "        \"\"\"Set Civitai API key\"\"\"\n",
    "        if key.strip():\n",
    "            self.civitai_key = key.strip()\n",
    "            os.environ['CIVITAI_API_KEY'] = self.civitai_key\n",
    "            self.status['civitai'] = True\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_headers(self, service: str = 'hf') -> Dict:\n",
    "        \"\"\"Get request headers for API calls\"\"\"\n",
    "        if service == 'hf' and self.hf_token:\n",
    "            return {'Authorization': f'Bearer {self.hf_token}'}\n",
    "        elif service == 'civitai' and self.civitai_key:\n",
    "            return {'Authorization': f'Bearer {self.civitai_key}'}\n",
    "        return {}\n",
    "\n",
    "# Initialize API Manager\n",
    "api_manager = APIKeysManager()\n",
    "\n",
    "# Create GUI for API keys\n",
    "hf_token_input = widgets.Password(\n",
    "    placeholder='Enter HuggingFace token',\n",
    "    description='HF Token:',\n",
    "    style={'description_width': '120px'},\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "civitai_key_input = widgets.Password(\n",
    "    placeholder='Enter Civitai API key',\n",
    "    description='Civitai Key:',\n",
    "    style={'description_width': '120px'},\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "hf_status = widgets.HTML(value=\"<i style='color:gray'>Not configured</i>\")\n",
    "civitai_status = widgets.HTML(value=\"<i style='color:gray'>Not configured</i>\")\n",
    "\n",
    "def save_hf_token(btn):\n",
    "    if api_manager.set_hf_token(hf_token_input.value):\n",
    "        hf_status.value = \"<b style='color:green'>‚úì Configured</b>\"\n",
    "    else:\n",
    "        hf_status.value = \"<b style='color:red'>‚úó Invalid token</b>\"\n",
    "\n",
    "def save_civitai_key(btn):\n",
    "    if api_manager.set_civitai_key(civitai_key_input.value):\n",
    "        civitai_status.value = \"<b style='color:green'>‚úì Configured</b>\"\n",
    "    else:\n",
    "        civitai_status.value = \"<b style='color:red'>‚úó Invalid key</b>\"\n",
    "\n",
    "hf_btn = widgets.Button(description='Save HF Token', button_style='info')\n",
    "hf_btn.on_click(save_hf_token)\n",
    "\n",
    "civitai_btn = widgets.Button(description='Save Civitai Key', button_style='info')\n",
    "civitai_btn.on_click(save_civitai_key)\n",
    "\n",
    "print(\"üîë API Keys Configuration\\n\")\n",
    "display(\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<h3>HuggingFace Token</h3>\"),\n",
    "        widgets.HBox([hf_token_input, hf_btn]),\n",
    "        hf_status,\n",
    "        widgets.HTML(\"<small><a href='https://huggingface.co/settings/tokens' target='_blank'>Get token from HuggingFace</a></small>\"),\n",
    "        \n",
    "        widgets.HTML(\"<hr>\"),\n",
    "        widgets.HTML(\"<h3>Civitai API Key</h3>\"),\n",
    "        widgets.HBox([civitai_key_input, civitai_btn]),\n",
    "        civitai_status,\n",
    "        widgets.HTML(\"<small><a href='https://civitai.com/user/account' target='_blank'>Get API key from Civitai</a></small>\"),\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646e9e43",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Model Download Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1aef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelDownloader:\n",
    "    \"\"\"Handle model downloads from various sources\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.checkpoint_dir = Path('./models/checkpoints')\n",
    "        self.lora_dir = Path('./models/loras')\n",
    "        self.vae_dir = Path('./models/vaes')\n",
    "    \n",
    "    def download_from_huggingface(self, model_id: str, hf_token: str = None) -> Optional[Path]:\n",
    "        \"\"\"Download model from HuggingFace Hub\"\"\"\n",
    "        try:\n",
    "            filepath = hf_hub_download(\n",
    "                repo_id=model_id,\n",
    "                filename=\"model.safetensors\",\n",
    "                cache_dir=str(self.checkpoint_dir),\n",
    "                token=hf_token,\n",
    "                force_download=False\n",
    "            )\n",
    "            print(f\"‚úÖ Downloaded: {Path(filepath).name}\")\n",
    "            return Path(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def download_from_url(self, url: str, output_path: Path) -> bool:\n",
    "        \"\"\"Download model from direct URL\"\"\"\n",
    "        import requests\n",
    "        try:\n",
    "            response = requests.get(url, stream=True, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            with open(output_path, 'wb') as f:\n",
    "                downloaded = 0\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        downloaded += len(chunk)\n",
    "                        if total_size:\n",
    "                            progress = (downloaded / total_size) * 100\n",
    "                            print(f\"\\rDownloading... {progress:.1f}%\", end='', flush=True)\n",
    "            print(f\"\\n‚úÖ Downloaded: {output_path.name}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def list_models(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"List all available models\"\"\"\n",
    "        models = {\n",
    "            'checkpoints': [f.name for f in self.checkpoint_dir.glob('*.safetensors') if f.is_file()],\n",
    "            'loras': [f.name for f in self.lora_dir.glob('*.safetensors') if f.is_file()],\n",
    "            'vaes': [f.name for f in self.vae_dir.glob('*.safetensors') if f.is_file()]\n",
    "        }\n",
    "        return models\n",
    "    \n",
    "    def get_model_size(self, filepath: Path) -> str:\n",
    "        \"\"\"Get human-readable file size\"\"\"\n",
    "        size_bytes = filepath.stat().st_size\n",
    "        for unit in ['B', 'KB', 'MB', 'GB']:\n",
    "            if size_bytes < 1024:\n",
    "                return f\"{size_bytes:.2f} {unit}\"\n",
    "            size_bytes /= 1024\n",
    "        return f\"{size_bytes:.2f} TB\"\n",
    "\n",
    "# Initialize downloader\n",
    "downloader = ModelDownloader()\n",
    "\n",
    "# Create GUI\n",
    "checkpoint_url = widgets.Text(\n",
    "    placeholder='e.g., runwayml/stable-diffusion-v1-5 or https://...',\n",
    "    description='Checkpoint:',\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "lora_url = widgets.Text(\n",
    "    placeholder='HuggingFace model ID or direct URL',\n",
    "    description='LoRA:',\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "vae_url = widgets.Text(\n",
    "    placeholder='VAE model URL',\n",
    "    description='VAE:',\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "output_download = widgets.Output()\n",
    "\n",
    "def download_checkpoint(btn):\n",
    "    with output_download:\n",
    "        clear_output()\n",
    "        url = checkpoint_url.value.strip()\n",
    "        if not url:\n",
    "            print(\"‚ùå Please enter a checkpoint URL\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üì• Downloading checkpoint: {url}...\")\n",
    "        if '/' in url and not url.startswith('http'):\n",
    "            downloader.download_from_huggingface(url, api_manager.hf_token)\n",
    "        else:\n",
    "            filename = url.split('/')[-1]\n",
    "            downloader.download_from_url(url, Path(downloader.checkpoint_dir) / filename)\n",
    "\n",
    "def download_lora(btn):\n",
    "    with output_download:\n",
    "        clear_output()\n",
    "        url = lora_url.value.strip()\n",
    "        if not url:\n",
    "            print(\"‚ùå Please enter a LoRA URL\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üì• Downloading LoRA: {url}...\")\n",
    "        if '/' in url and not url.startswith('http'):\n",
    "            downloader.download_from_huggingface(url, api_manager.hf_token)\n",
    "        else:\n",
    "            filename = url.split('/')[-1]\n",
    "            downloader.download_from_url(url, Path(downloader.lora_dir) / filename)\n",
    "\n",
    "def download_vae(btn):\n",
    "    with output_download:\n",
    "        clear_output()\n",
    "        url = vae_url.value.strip()\n",
    "        if not url:\n",
    "            print(\"‚ùå Please enter a VAE URL\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üì• Downloading VAE: {url}...\")\n",
    "        if '/' in url and not url.startswith('http'):\n",
    "            downloader.download_from_huggingface(url, api_manager.hf_token)\n",
    "        else:\n",
    "            filename = url.split('/')[-1]\n",
    "            downloader.download_from_url(url, Path(downloader.vae_dir) / filename)\n",
    "\n",
    "checkpoint_btn = widgets.Button(description='Download', button_style='success')\n",
    "checkpoint_btn.on_click(download_checkpoint)\n",
    "\n",
    "lora_btn = widgets.Button(description='Download', button_style='success')\n",
    "lora_btn.on_click(download_lora)\n",
    "\n",
    "vae_btn = widgets.Button(description='Download', button_style='success')\n",
    "vae_btn.on_click(download_vae)\n",
    "\n",
    "print(\"üì¶ Model Download Manager\\n\")\n",
    "display(\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Download Checkpoint</h3>\"),\n",
    "        widgets.HBox([checkpoint_url, checkpoint_btn]),\n",
    "        \n",
    "        widgets.HTML(\"<h3>Download LoRA</h3>\"),\n",
    "        widgets.HBox([lora_url, lora_btn]),\n",
    "        \n",
    "        widgets.HTML(\"<h3>Download VAE</h3>\"),\n",
    "        widgets.HBox([vae_url, vae_btn]),\n",
    "        \n",
    "        output_download\n",
    "    ])\n",
    ")\n",
    "\n",
    "# List available models\n",
    "print(\"\\nüìÇ Available Models:\")\n",
    "models = downloader.list_models()\n",
    "for model_type, files in models.items():\n",
    "    print(f\"\\n{model_type.upper()}:\")\n",
    "    if files:\n",
    "        for f in files:\n",
    "            path = {\n",
    "                'checkpoints': downloader.checkpoint_dir,\n",
    "                'loras': downloader.lora_dir,\n",
    "                'vaes': downloader.vae_dir\n",
    "            }[model_type] / f\n",
    "            size = downloader.get_model_size(path)\n",
    "            print(f\"  - {f} ({size})\")\n",
    "    else:\n",
    "        print(f\"  (No models downloaded)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504420a0",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Checkpoint and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317d3e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StableDiffusionManager:\n",
    "    \"\"\"Manage Stable Diffusion models and pipelines\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.pipe = None\n",
    "        self.current_model = None\n",
    "        self.loras = {}\n",
    "        self.vae = None\n",
    "    \n",
    "    def load_model(self, model_id: str, hf_token: str = None, precision: str = 'fp16'):\n",
    "        \"\"\"Load a Stable Diffusion model\"\"\"\n",
    "        try:\n",
    "            print(f\"üîÑ Loading model: {model_id}...\")\n",
    "            \n",
    "            torch_dtype = torch.float16 if precision == 'fp16' else torch.float32\n",
    "            \n",
    "            # Load based on model type\n",
    "            if 'xl' in model_id.lower():\n",
    "                pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "                    model_id,\n",
    "                    torch_dtype=torch_dtype,\n",
    "                    use_safetensors=True,\n",
    "                    use_auth_token=hf_token\n",
    "                )\n",
    "            else:\n",
    "                pipe = StableDiffusionPipeline.from_pretrained(\n",
    "                    model_id,\n",
    "                    torch_dtype=torch_dtype,\n",
    "                    use_safetensors=True,\n",
    "                    use_auth_token=hf_token\n",
    "                )\n",
    "            \n",
    "            pipe = pipe.to(self.device)\n",
    "            \n",
    "            # Enable optimizations\n",
    "            pipe.enable_attention_slicing()\n",
    "            if hasattr(pipe, 'enable_xformers_memory_efficient_attention'):\n",
    "                try:\n",
    "                    pipe.enable_xformers_memory_efficient_attention()\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            self.pipe = pipe\n",
    "            self.current_model = model_id\n",
    "            print(f\"‚úÖ Model loaded successfully!\")\n",
    "            return True\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading model: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def load_lora(self, lora_path: str, weight: float = 1.0):\n",
    "        \"\"\"Load LoRA model\"\"\"\n",
    "        try:\n",
    "            if not self.pipe:\n",
    "                print(\"‚ùå Load a checkpoint first\")\n",
    "                return False\n",
    "            \n",
    "            self.pipe.load_lora_weights(lora_path)\n",
    "            self.pipe.set_lora_device(self.device)\n",
    "            self.pipe.fuse_lora(lora_scale=weight)\n",
    "            print(f\"‚úÖ LoRA loaded: {Path(lora_path).name}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading LoRA: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def load_vae(self, vae_path: str):\n",
    "        \"\"\"Load custom VAE\"\"\"\n",
    "        try:\n",
    "            if not self.pipe:\n",
    "                print(\"‚ùå Load a checkpoint first\")\n",
    "                return False\n",
    "            \n",
    "            vae = AutoencoderKL.from_single_file(vae_path)\n",
    "            self.pipe.vae = vae.to(self.device)\n",
    "            print(f\"‚úÖ VAE loaded: {Path(vae_path).name}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading VAE: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        negative_prompt: str = \"\",\n",
    "        height: int = 512,\n",
    "        width: int = 512,\n",
    "        num_inference_steps: int = 20,\n",
    "        guidance_scale: float = 7.5,\n",
    "        seed: int = -1,\n",
    "        num_images: int = 1\n",
    "    ) -> List[Image.Image]:\n",
    "        \"\"\"Generate images\"\"\"\n",
    "        try:\n",
    "            if not self.pipe:\n",
    "                print(\"‚ùå Load a model first\")\n",
    "                return []\n",
    "            \n",
    "            if seed >= 0:\n",
    "                generator = torch.Generator(device=self.device).manual_seed(seed)\n",
    "            else:\n",
    "                generator = None\n",
    "            \n",
    "            print(f\"üé® Generating {num_images} image(s)...\")\n",
    "            \n",
    "            result = self.pipe(\n",
    "                prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                height=height,\n",
    "                width=width,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                num_images_per_prompt=num_images,\n",
    "                generator=generator\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Generation complete!\")\n",
    "            return result.images\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Generation error: {e}\")\n",
    "            return []\n",
    "\n",
    "# Initialize manager\n",
    "sd_manager = StableDiffusionManager()\n",
    "\n",
    "# Create model selection GUI\n",
    "model_options = [\n",
    "    'runwayml/stable-diffusion-v1-5',\n",
    "    'stabilityai/stable-diffusion-2-1',\n",
    "    'stabilityai/stable-diffusion-xl-base-1.0'\n",
    "]\n",
    "\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=model_options,\n",
    "    value=model_options[0],\n",
    "    description='Model:',\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "precision_radio = widgets.RadioButtons(\n",
    "    options=['fp16 (fast)', 'fp32 (accurate)'],\n",
    "    description='Precision:'\n",
    ")\n",
    "\n",
    "load_output = widgets.Output()\n",
    "\n",
    "def load_model_click(btn):\n",
    "    with load_output:\n",
    "        clear_output()\n",
    "        precision = 'fp16' if 'fp16' in precision_radio.value else 'fp32'\n",
    "        sd_manager.load_model(model_dropdown.value, api_manager.hf_token, precision)\n",
    "\n",
    "load_btn = widgets.Button(description='Load Model', button_style='warning')\n",
    "load_btn.on_click(load_model_click)\n",
    "\n",
    "print(\"‚öôÔ∏è Model Configuration\\n\")\n",
    "display(\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Select Model</h3>\"),\n",
    "        model_dropdown,\n",
    "        precision_radio,\n",
    "        widgets.HBox([load_btn]),\n",
    "        load_output\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793be24e",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Generate Images with Full Parameter Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989e6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generation GUI\n",
    "prompt_input = widgets.Textarea(\n",
    "    placeholder='Describe what you want to generate...',\n",
    "    description='Prompt:',\n",
    "    rows=4,\n",
    "    layout=widgets.Layout(width='100%')\n",
    ")\n",
    "\n",
    "negative_prompt_input = widgets.Textarea(\n",
    "    placeholder='What to avoid...',\n",
    "    description='Negative:',\n",
    "    rows=3,\n",
    "    layout=widgets.Layout(width='100%')\n",
    ")\n",
    "\n",
    "# Generation parameters\n",
    "width_input = widgets.IntSlider(min=64, max=2048, step=64, value=512, description='Width:')\n",
    "height_input = widgets.IntSlider(min=64, max=2048, step=64, value=512, description='Height:')\n",
    "steps_input = widgets.IntSlider(min=1, max=100, value=20, description='Steps:')\n",
    "cfg_input = widgets.FloatSlider(min=1, max=20, step=0.5, value=7.5, description='CFG Scale:')\n",
    "seed_input = widgets.IntSlider(min=-1, max=2147483647, value=-1, description='Seed:')\n",
    "num_images_input = widgets.IntSlider(min=1, max=4, value=1, description='Num Images:')\n",
    "\n",
    "sampler_dropdown = widgets.Dropdown(\n",
    "    options=['euler', 'ddim', 'pndm', 'lms'],\n",
    "    value='euler',\n",
    "    description='Sampler:'\n",
    ")\n",
    "\n",
    "# Generation output\n",
    "gen_output = widgets.Output()\n",
    "\n",
    "def generate_click(btn):\n",
    "    with gen_output:\n",
    "        clear_output()\n",
    "        \n",
    "        prompt = prompt_input.value.strip()\n",
    "        if not prompt:\n",
    "            print(\"‚ùå Please enter a prompt\")\n",
    "            return\n",
    "        \n",
    "        if not sd_manager.pipe:\n",
    "            print(\"‚ùå Load a model first\")\n",
    "            return\n",
    "        \n",
    "        # Set scheduler\n",
    "        if sampler_dropdown.value == 'euler':\n",
    "            sd_manager.pipe.scheduler = EulerDiscreteScheduler.from_config(sd_manager.pipe.scheduler.config)\n",
    "        elif sampler_dropdown.value == 'ddim':\n",
    "            sd_manager.pipe.scheduler = DDIMScheduler.from_config(sd_manager.pipe.scheduler.config)\n",
    "        elif sampler_dropdown.value == 'pndm':\n",
    "            sd_manager.pipe.scheduler = PNDMScheduler.from_config(sd_manager.pipe.scheduler.config)\n",
    "        \n",
    "        # Generate\n",
    "        start_time = time.time()\n",
    "        images = sd_manager.generate(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt_input.value,\n",
    "            height=int(height_input.value),\n",
    "            width=int(width_input.value),\n",
    "            num_inference_steps=int(steps_input.value),\n",
    "            guidance_scale=float(cfg_input.value),\n",
    "            seed=int(seed_input.value),\n",
    "            num_images=int(num_images_input.value)\n",
    "        )\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        if images:\n",
    "            # Display images\n",
    "            print(f\"\\n‚è±Ô∏è Time: {elapsed:.2f}s\\n\")\n",
    "            \n",
    "            # Create grid\n",
    "            cols = min(len(images), 2)\n",
    "            rows = (len(images) + cols - 1) // cols\n",
    "            grid = Image.new('RGB', (width_input.value * cols, height_input.value * rows))\n",
    "            \n",
    "            for idx, img in enumerate(images):\n",
    "                x = (idx % cols) * width_input.value\n",
    "                y = (idx // cols) * height_input.value\n",
    "                grid.paste(img, (x, y))\n",
    "                \n",
    "                # Save\n",
    "                output_path = Path('./outputs') / f\"gen_{int(time.time())}_{idx}.png\"\n",
    "                img.save(output_path)\n",
    "                print(f\"üíæ Saved: {output_path.name}\")\n",
    "            \n",
    "            display(grid)\n",
    "\n",
    "generate_btn = widgets.Button(description='üé® Generate', button_style='success', button_width='150px')\n",
    "generate_btn.on_click(generate_click)\n",
    "\n",
    "print(\"üé® Image Generation\\n\")\n",
    "display(\n",
    "    widgets.VBox([\n",
    "        prompt_input,\n",
    "        negative_prompt_input,\n",
    "        widgets.HBox([width_input, height_input]),\n",
    "        widgets.HBox([steps_input, cfg_input]),\n",
    "        widgets.HBox([seed_input, num_images_input]),\n",
    "        widgets.HBox([sampler_dropdown]),\n",
    "        generate_btn,\n",
    "        gen_output\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566b256c",
   "metadata": {},
   "source": [
    "## üéØ Quick Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfcaeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generate beautiful landscape\n",
    "if sd_manager.pipe:\n",
    "    example_prompt = \"a beautiful landscape with mountains and lake, sunset, highly detailed, 4k resolution\"\n",
    "    example_negative = \"low quality, blurry, distorted\"\n",
    "    \n",
    "    print(f\"üìù Example prompt: {example_prompt}\")\n",
    "    print(f\"\\nTo generate:\")\n",
    "    print(f\"1. Paste the prompt above\")\n",
    "    print(f\"2. Adjust parameters as needed\")\n",
    "    print(f\"3. Click 'Generate' button\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Load a model first to generate images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32d53da",
   "metadata": {},
   "source": [
    "## üìä Gallery & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd7ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gallery():\n",
    "    \"\"\"Display all generated images\"\"\"\n",
    "    output_dir = Path('./outputs')\n",
    "    images = sorted(output_dir.glob('*.png'), key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    if not images:\n",
    "        print(\"üì≠ No images generated yet\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üì∏ Gallery ({len(images)} images)\\n\")\n",
    "    \n",
    "    # Display in grid\n",
    "    cols = 3\n",
    "    rows = (len(images) + cols - 1) // cols\n",
    "    \n",
    "    for i in range(0, len(images), cols):\n",
    "        batch = images[i:i+cols]\n",
    "        fig_width = 15\n",
    "        fig_height = 5 * rows\n",
    "        \n",
    "        for img_path in batch:\n",
    "            img = Image.open(img_path)\n",
    "            print(f\"\\n{img_path.name} - {img.size}\")\n",
    "            display(img)\n",
    "\n",
    "# Show gallery\n",
    "show_gallery()\n",
    "\n",
    "# Download all\n",
    "def download_all():\n",
    "    \"\"\"Create downloadable archive\"\"\"\n",
    "    import zipfile\n",
    "    output_dir = Path('./outputs')\n",
    "    if output_dir.exists() and list(output_dir.glob('*.png')):\n",
    "        zip_path = Path('./generated_images.zip')\n",
    "        with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "            for img in output_dir.glob('*.png'):\n",
    "                zipf.write(img, img.name)\n",
    "        print(f\"‚úÖ Archive created: {zip_path.name}\")\n",
    "        print(f\"üì¶ Size: {zip_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "        return zip_path\n",
    "    return None\n",
    "\n",
    "print(\"\\nüíæ To download all generated images, run the next cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe31f414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all images\n",
    "zip_file = download_all()\n",
    "\n",
    "if zip_file:\n",
    "    from IPython.display import FileLink\n",
    "    display(FileLink(str(zip_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0400c1e",
   "metadata": {},
   "source": [
    "## üöÄ Tips & Tricks\n",
    "\n",
    "- **Better Quality**: Increase steps (20-50) and CFG scale (7-15)\n",
    "- **Faster Generation**: Lower steps (10-15) and use fp16 precision\n",
    "- **LoRA Enhancement**: Download LoRA models for style customization\n",
    "- **Memory Issues**: Enable memory efficient attention and CPU offload\n",
    "- **Custom Seeds**: Use negative seed values for random generation\n",
    "- **Batch Generation**: Set num_images > 1 to generate multiple variations\n",
    "\n",
    "---\n",
    "\n",
    "**Enjoy generating amazing images! üé®**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
